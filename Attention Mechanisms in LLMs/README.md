# Attention Mechanisms in Transformers and Transformer like architectures, games with local Large-Language-Models  
  
How does attention work? Multi-headed attention?  
Building a series of notebooks to capture the various ideas around attention.  
  
  

