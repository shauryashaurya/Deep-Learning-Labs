# Tokenization and Sub-word Splitting - Encoder, Decoder architectures, Large-Language-Models and other needs  
      
Byte-pair-encoding?     
Variational Autoencoders?    
What is Tokenization and how is it working?    
With Large-Language-Models this is becoming increasingly opaque and arcane.   
Think about context, how it may differ for the same "token" in the same "prompt" - this is getting too much!   
Here in these series of experiments, I am going to try and gain some insight into Tokenization.     
   

